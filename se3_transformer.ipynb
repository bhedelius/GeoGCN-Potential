{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "se3-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulxwtkGl_5aK"
      },
      "source": [
        "# Install libraries\r\n",
        "!pip install --pre dgl-cu101\r\n",
        "\r\n",
        "# Import libraries\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import dgl\r\n",
        "import dgl.function as fn\r\n",
        "from dgl import DGLGraph\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "\r\n",
        "!pip install git+https://github.com/AMLab-Amsterdam/lie_learn\r\n",
        "import lie_learn\r\n",
        "\r\n",
        "from lie_learn.representations.SO3.wigner_d import wigner_D_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmB9gct0kvJO"
      },
      "source": [
        "def get_spherical_from_cartesian_torch(cartesian, divide_radius_by=1.0):\r\n",
        "\r\n",
        "    ###################################################################################################################\r\n",
        "    # ON ANGLE CONVENTION\r\n",
        "    #\r\n",
        "    # sh has following convention for angles:\r\n",
        "    # :param theta: the colatitude / polar angle, ranging from 0(North Pole, (X, Y, Z) = (0, 0, 1)) to pi(South Pole, (X, Y, Z) = (0, 0, -1)).\r\n",
        "    # :param phi: the longitude / azimuthal angle, ranging from 0 to 2 pi.\r\n",
        "    #\r\n",
        "    # the 3D steerable CNN code therefore (probably) has the following convention for alpha and beta:\r\n",
        "    # beta = pi - theta; ranging from 0(South Pole, (X, Y, Z) = (0, 0, -1)) to pi(North Pole, (X, Y, Z) = (0, 0, 1)).\r\n",
        "    # alpha = phi\r\n",
        "    #\r\n",
        "    ###################################################################################################################\r\n",
        "\r\n",
        "    # initialise return array\r\n",
        "    # ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\r\n",
        "    spherical = torch.zeros_like(cartesian)\r\n",
        "\r\n",
        "    # indices for return array\r\n",
        "    ind_radius = 0\r\n",
        "    ind_alpha = 1\r\n",
        "    ind_beta = 2\r\n",
        "\r\n",
        "    cartesian_x = 2\r\n",
        "    cartesian_y = 0\r\n",
        "    cartesian_z = 1\r\n",
        "\r\n",
        "    # get projected radius in xy plane\r\n",
        "    # xy = xyz[:,0]**2 + xyz[:,1]**2\r\n",
        "    r_xy = cartesian[..., cartesian_x] ** 2 + cartesian[..., cartesian_y] ** 2\r\n",
        "\r\n",
        "    # get second angle\r\n",
        "    # version 'elevation angle defined from Z-axis down'\r\n",
        "    spherical[..., ind_beta] = torch.atan2(torch.sqrt(r_xy), cartesian[..., cartesian_z])\r\n",
        "    # ptsnew[:,4] = np.arctan2(np.sqrt(xy), xyz[:,2])\r\n",
        "    # version 'elevation angle defined from XY-plane up'\r\n",
        "    #ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy))\r\n",
        "    # spherical[:, ind_beta] = np.arctan2(cartesian[:, 2], np.sqrt(r_xy))\r\n",
        "\r\n",
        "    # get angle in x-y plane\r\n",
        "    spherical[...,ind_alpha] = torch.atan2(cartesian[...,cartesian_y], cartesian[...,cartesian_x])\r\n",
        "\r\n",
        "    # get overall radius\r\n",
        "    # ptsnew[:,3] = np.sqrt(xy + xyz[:,2]**2)\r\n",
        "    if divide_radius_by == 1.0:\r\n",
        "        spherical[..., ind_radius] = torch.sqrt(r_xy + cartesian[...,cartesian_z]**2)\r\n",
        "    else:\r\n",
        "        spherical[..., ind_radius] = torch.sqrt(r_xy + cartesian[...,cartesian_z]**2)/divide_radius_by\r\n",
        "\r\n",
        "    return spherical\r\n",
        "\r\n",
        "def pochhammer(x, k):\r\n",
        "    \"\"\"Compute the pochhammer symbol (x)_k.\r\n",
        "    (x)_k = x * (x+1) * (x+2) *...* (x+k-1)\r\n",
        "    Args:\r\n",
        "        x: positive int\r\n",
        "    Returns:\r\n",
        "        float for (x)_k\r\n",
        "    \"\"\"\r\n",
        "    xf = float(x)\r\n",
        "    for n in range(x+1, x+k):\r\n",
        "        xf *= n\r\n",
        "    return xf\r\n",
        "    \r\n",
        "def semifactorial(x):\r\n",
        "    \"\"\"Compute the semifactorial function x!!.\r\n",
        "    x!! = x * (x-2) * (x-4) *...\r\n",
        "    Args:\r\n",
        "        x: positive int\r\n",
        "    Returns:\r\n",
        "        float for x!!\r\n",
        "    \"\"\"\r\n",
        "    y = 1.\r\n",
        "    for n in range(x, 1, -2):\r\n",
        "        y *= n\r\n",
        "    return y\r\n",
        "\r\n",
        "class SphericalHarmonics(object):\r\n",
        "    def __init__(self):\r\n",
        "        self.leg = {}\r\n",
        "\r\n",
        "    def clear(self):\r\n",
        "        self.leg = {}\r\n",
        "\r\n",
        "    def negative_lpmv(self, l, m, y):\r\n",
        "        \"\"\"Compute negative order coefficients\"\"\"\r\n",
        "        if m < 0:\r\n",
        "            y *= ((-1)**m / pochhammer(l+m+1, -2*m))\r\n",
        "        return y\r\n",
        "\r\n",
        "    def lpmv(self, l, m, x):\r\n",
        "        \"\"\"Associated Legendre function including Condon-Shortley phase.\r\n",
        "        Args:\r\n",
        "            m: int order \r\n",
        "            l: int degree\r\n",
        "            x: float argument tensor\r\n",
        "        Returns:\r\n",
        "            tensor of x-shape\r\n",
        "        \"\"\"\r\n",
        "        # Check memoized versions\r\n",
        "        m_abs = abs(m)\r\n",
        "        if (l,m) in self.leg:\r\n",
        "            return self.leg[(l,m)]\r\n",
        "        elif m_abs > l:\r\n",
        "            return None\r\n",
        "        elif l == 0:\r\n",
        "            self.leg[(l,m)] = torch.ones_like(x)\r\n",
        "            return self.leg[(l,m)]\r\n",
        "        \r\n",
        "        # Check if on boundary else recurse solution down to boundary\r\n",
        "        if m_abs == l:\r\n",
        "            # Compute P_m^m\r\n",
        "            y = (-1)**m_abs * semifactorial(2*m_abs-1)\r\n",
        "            y *= torch.pow(1-x*x, m_abs/2)\r\n",
        "            self.leg[(l,m)] = self.negative_lpmv(l, m, y)\r\n",
        "            return self.leg[(l,m)]\r\n",
        "        else:\r\n",
        "            # Recursively precompute lower degree harmonics\r\n",
        "            self.lpmv(l-1, m, x)\r\n",
        "\r\n",
        "        # Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\r\n",
        "        # Inplace speedup\r\n",
        "        y = ((2*l-1) / (l-m_abs)) * x * self.lpmv(l-1, m_abs, x)\r\n",
        "        if l - m_abs > 1:\r\n",
        "            y -= ((l+m_abs-1)/(l-m_abs)) * self.leg[(l-2, m_abs)]\r\n",
        "        #self.leg[(l, m_abs)] = y\r\n",
        "        \r\n",
        "        if m < 0:\r\n",
        "            y = self.negative_lpmv(l, m, y)\r\n",
        "        self.leg[(l,m)] = y\r\n",
        "\r\n",
        "        return self.leg[(l,m)]\r\n",
        "\r\n",
        "    def get_element(self, l, m, theta, phi):\r\n",
        "        \"\"\"Tesseral spherical harmonic with Condon-Shortley phase.\r\n",
        "        The Tesseral spherical harmonics are also known as the real spherical\r\n",
        "        harmonics.\r\n",
        "        Args:\r\n",
        "            l: int for degree\r\n",
        "            m: int for order, where -l <= m < l\r\n",
        "            theta: collatitude or polar angle\r\n",
        "            phi: longitude or azimuth\r\n",
        "        Returns:\r\n",
        "            tensor of shape theta\r\n",
        "        \"\"\"\r\n",
        "        assert abs(m) <= l, \"absolute value of order m must be <= degree l\"\r\n",
        "\r\n",
        "        N = np.sqrt((2*l+1) / (4*np.pi))\r\n",
        "        leg = self.lpmv(l, abs(m), torch.cos(theta))\r\n",
        "        if m == 0:\r\n",
        "            return N*leg\r\n",
        "        elif m > 0:\r\n",
        "            Y = torch.cos(m*phi) * leg\r\n",
        "        else:\r\n",
        "            Y = torch.sin(abs(m)*phi) * leg\r\n",
        "        N *= np.sqrt(2. / pochhammer(l-abs(m)+1, 2*abs(m)))\r\n",
        "        Y *= N\r\n",
        "        return Y\r\n",
        "\r\n",
        "    def get(self, l, theta, phi, refresh=True):\r\n",
        "        \"\"\"Tesseral harmonic with Condon-Shortley phase.\r\n",
        "        The Tesseral spherical harmonics are also known as the real spherical\r\n",
        "        harmonics.\r\n",
        "        Args:\r\n",
        "            l: int for degree\r\n",
        "            theta: collatitude or polar angle\r\n",
        "            phi: longitude or azimuth\r\n",
        "        Returns:\r\n",
        "            tensor of shape [*theta.shape, 2*l+1]\r\n",
        "        \"\"\"\r\n",
        "        results = []\r\n",
        "        if refresh:\r\n",
        "            self.clear()\r\n",
        "        for m in range(-l, l+1):\r\n",
        "            results.append(self.get_element(l, m, theta, phi))\r\n",
        "        return torch.stack(results, -1)\r\n",
        "\r\n",
        "def precompute_sh(r_ij, max_J):\r\n",
        "    \"\"\"\r\n",
        "    pre-comput spherical harmonics up to order max_J\r\n",
        "    :param r_ij: relative positions\r\n",
        "    :param max_J: maximum order used in entire network\r\n",
        "    :return: dict where each entry has shape [B,N,K,2J+1]\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    i_distance = 0\r\n",
        "    i_alpha = 1\r\n",
        "    i_beta = 2\r\n",
        "\r\n",
        "    Y_Js = {}\r\n",
        "    sh = SphericalHarmonics()\r\n",
        "\r\n",
        "    for J in range(max_J+1):\r\n",
        "        # dimension [B,N,K,2J+1]\r\n",
        "        #Y_Js[J] = spherical_harmonics(order=J, alpha=r_ij[...,i_alpha], beta=r_ij[...,i_beta])\r\n",
        "        Y_Js[J] = sh.get(J, theta=math.pi-r_ij[...,i_beta], phi=r_ij[...,i_alpha], refresh=False)\r\n",
        "\r\n",
        "    sh.clear()\r\n",
        "    return Y_Js\r\n",
        "\r\n",
        "def irr_repr(order, alpha, beta, gamma, dtype=None):\r\n",
        "    \"\"\"\r\n",
        "    irreducible representation of SO3\r\n",
        "    - compatible with compose and spherical_harmonics\r\n",
        "    \"\"\"\r\n",
        "    # from from_lielearn_SO3.wigner_d import wigner_D_matrix\r\n",
        "    from lie_learn.representations.SO3.wigner_d import wigner_D_matrix\r\n",
        "    # if order == 1:\r\n",
        "    #     # change of basis to have vector_field[x, y, z] = [vx, vy, vz]\r\n",
        "    #     A = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\r\n",
        "    #     return A @ wigner_D_matrix(1, alpha, beta, gamma) @ A.T\r\n",
        "\r\n",
        "    # TODO (non-essential): try to do everything in torch\r\n",
        "    # return torch.tensor(wigner_D_matrix(torch.tensor(order), alpha, beta, gamma), dtype=torch.get_default_dtype() if dtype is None else dtype)\r\n",
        "    return torch.tensor(wigner_D_matrix(order, np.array(alpha), np.array(beta), np.array(gamma)), dtype=torch.get_default_dtype() if dtype is None else dtype)\r\n",
        "\r\n",
        "\r\n",
        "def kron(a, b):\r\n",
        "    \"\"\"\r\n",
        "    A part of the pylabyk library: numpytorch.py at https://github.com/yulkang/pylabyk\r\n",
        "    Kronecker product of matrices a and b with leading batch dimensions.\r\n",
        "    Batch dimensions are broadcast. The number of them mush\r\n",
        "    :type a: torch.Tensor\r\n",
        "    :type b: torch.Tensor\r\n",
        "    :rtype: torch.Tensor\r\n",
        "    \"\"\"\r\n",
        "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\r\n",
        "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\r\n",
        "    siz0 = res.shape[:-4]\r\n",
        "    return res.reshape(siz0 + siz1)\r\n",
        "\r\n",
        "def get_matrix_kernel(A, eps=1e-10):\r\n",
        "    '''\r\n",
        "    Compute an orthonormal basis of the kernel (x_1, x_2, ...)\r\n",
        "    A x_i = 0\r\n",
        "    scalar_product(x_i, x_j) = delta_ij\r\n",
        "    :param A: matrix\r\n",
        "    :return: matrix where each row is a basis vector of the kernel of A\r\n",
        "    '''\r\n",
        "    _u, s, v = torch.svd(A)\r\n",
        "\r\n",
        "    # A = u @ torch.diag(s) @ v.t()\r\n",
        "    kernel = v.t()[s < eps]\r\n",
        "    return kernel\r\n",
        "\r\n",
        "\r\n",
        "def get_matrices_kernel(As, eps=1e-10):\r\n",
        "    '''\r\n",
        "    Computes the commun kernel of all the As matrices\r\n",
        "    '''\r\n",
        "    return get_matrix_kernel(torch.cat(As, dim=0), eps)\r\n",
        "    \r\n",
        "class torch_default_dtype:\r\n",
        "\r\n",
        "    def __init__(self, dtype):\r\n",
        "        self.saved_dtype = None\r\n",
        "        self.dtype = dtype\r\n",
        "\r\n",
        "    def __enter__(self):\r\n",
        "        self.saved_dtype = torch.get_default_dtype()\r\n",
        "        torch.set_default_dtype(self.dtype)\r\n",
        "\r\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\r\n",
        "        torch.set_default_dtype(self.saved_dtype)\r\n",
        "\r\n",
        "def _basis_transformation_Q_J(J, order_in, order_out, version=3):  # pylint: disable=W0613\r\n",
        "    \"\"\"\r\n",
        "    :param J: order of the spherical harmonics\r\n",
        "    :param order_in: order of the input representation\r\n",
        "    :param order_out: order of the output representation\r\n",
        "    :return: one part of the Q^-1 matrix of the article\r\n",
        "    \"\"\"\r\n",
        "    with torch_default_dtype(torch.float64):\r\n",
        "        def _R_tensor(a, b, c): return kron(irr_repr(order_out, a, b, c), irr_repr(order_in, a, b, c))\r\n",
        "\r\n",
        "        def _sylvester_submatrix(J, a, b, c):\r\n",
        "            ''' generate Kronecker product matrix for solving the Sylvester equation in subspace J '''\r\n",
        "            R_tensor = _R_tensor(a, b, c)  # [m_out * m_in, m_out * m_in]\r\n",
        "            R_irrep_J = irr_repr(J, a, b, c)  # [m, m]\r\n",
        "            return kron(R_tensor, torch.eye(R_irrep_J.size(0))) - \\\r\n",
        "                kron(torch.eye(R_tensor.size(0)), R_irrep_J.t())  # [(m_out * m_in) * m, (m_out * m_in) * m]\r\n",
        "\r\n",
        "        random_angles = [\r\n",
        "            [4.41301023, 5.56684102, 4.59384642],\r\n",
        "            [4.93325116, 6.12697327, 4.14574096],\r\n",
        "            [0.53878964, 4.09050444, 5.36539036],\r\n",
        "            [2.16017393, 3.48835314, 5.55174441],\r\n",
        "            [2.52385107, 0.2908958, 3.90040975]\r\n",
        "        ]\r\n",
        "        null_space = get_matrices_kernel([_sylvester_submatrix(J, a, b, c) for a, b, c in random_angles])\r\n",
        "        assert null_space.size(0) == 1, null_space.size()  # unique subspace solution\r\n",
        "        Q_J = null_space[0]  # [(m_out * m_in) * m]\r\n",
        "        Q_J = Q_J.view((2 * order_out + 1) * (2 * order_in + 1), 2 * J + 1)  # [m_out * m_in, m]\r\n",
        "        assert all(torch.allclose(_R_tensor(a, b, c) @ Q_J, Q_J @ irr_repr(J, a, b, c)) for a, b, c in torch.rand(4, 3))\r\n",
        "\r\n",
        "    assert Q_J.dtype == torch.float64\r\n",
        "    return Q_J  # [m_out * m_in, m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDfUPE4SKm90"
      },
      "source": [
        "class PointCloud:\r\n",
        "  '''Represents a point cloud in R^3. This class calculates and stores relevant\r\n",
        "  geometric information such as the vectors, distances, directions, and\r\n",
        "  spherical harmonics of the vectors.'''\r\n",
        "  def __init__(self, pos, cutoff=8.0, J_max=4):\r\n",
        "    edges = self._find_edges_(pos, cutoff)\r\n",
        "    self.graph = dgl.graph(edges)\r\n",
        "    self.graph.ndata['pos'] = pos\r\n",
        "    self._calc_edge_info_(J_max)\r\n",
        "    self.w_j = dict()\r\n",
        "\r\n",
        "  def _find_edges_(self, pos, cutoff):\r\n",
        "    # Use positions to create graph. Need to improve! Currently O(n^2)\r\n",
        "    vec_mat = pos[:,None,:]-pos[None,:,:]\r\n",
        "    dist_mat = torch.sqrt(torch.sum((vec_mat)**2,axis=-1))\r\n",
        "    u = []\r\n",
        "    v = []\r\n",
        "    for j in range(len(pos)):\r\n",
        "      for i in range(j):\r\n",
        "        if dist_mat[i,j] < cutoff:\r\n",
        "          u.append(i)\r\n",
        "          v.append(j)\r\n",
        "    u, v = torch.tensor(u+v), torch.tensor(v+u)\r\n",
        "    return (u,v)\r\n",
        "\r\n",
        "  def _calc_edge_info_(self, J_max):\r\n",
        "    # Calculate and store position and angle information\r\n",
        "    u,v = self.graph.edges()[0], self.graph.edges()[1]\r\n",
        "    pos = self.graph.ndata['pos']\r\n",
        "    vec = pos[u]-pos[v]\r\n",
        "    self.graph.edata['vec'] = vec\r\n",
        "    r_ij = get_spherical_from_cartesian_torch(vec)\r\n",
        "    self.graph.edata['r_ij'] = r_ij\r\n",
        "    self.Y = precompute_sh(r_ij, J_max)\r\n",
        "    self.graph.edata['dist'] = torch.norm(vec, dim=1)\r\n",
        "\r\n",
        "  def get_sh(self, J):\r\n",
        "    # Returns spherical harmonic of order J.\r\n",
        "    if not J in self.Y.keys(): # If J <= J_max this is false.\r\n",
        "      r_ij = self.graph.edata['r_ij']\r\n",
        "      Y_new = precompute_sh(r_ij,J)\r\n",
        "      for key in Y_new.keys():\r\n",
        "        if not key in self.Y.keys():\r\n",
        "          self.Y[key] = Y_new[key]\r\n",
        "    return self.Y[J]\r\n",
        "    \r\n",
        "  def get_w_j(self, l, k):\r\n",
        "    # Returns basis kernel\r\n",
        "    if not (l,k) in self.w_j.keys():\r\n",
        "      w_j = torch.zeros(k+l-abs(k-l)+1, self.graph.number_of_edges(), 2*l+1, 2*k+1)\r\n",
        "      for j, J in enumerate(range(abs(k-l), k+l+1)):\r\n",
        "        Y_J = self.get_sh(J)\r\n",
        "        Q_J = _basis_transformation_Q_J(J, k, l).float()\r\n",
        "        w_j[j] = torch.matmul(Y_J,Q_J.T).reshape(self.graph.number_of_edges(), 2*l+1, 2*k+1)#.transpose(2,3)\r\n",
        "      self.w_j[(l,k)] = w_j.transpose(0,1)\r\n",
        "    return self.w_j[(l,k)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjZzy5_3SIdK"
      },
      "source": [
        "# TODO: Self interaction\r\n",
        "# TODO: Ability to change head size\r\n",
        "\r\n",
        "# Dictionary for indices\r\n",
        "# e: edges\r\n",
        "# o: c_out\r\n",
        "# i: c_in\r\n",
        "# l: output tensor representation\r\n",
        "# k: input tensor representation\r\n",
        "# j: hidden tensor representation\r\n",
        "\r\n",
        "class WLayer(nn.Module):\r\n",
        "  def __init__(self, k, l, c_in=1, c_out=1):\r\n",
        "    super(WLayer, self).__init__()\r\n",
        "    self.k = k\r\n",
        "    self.l = l\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "\r\n",
        "    J_size = k+l-abs(k-l)+1\r\n",
        "    self.J_size = J_size\r\n",
        "    r_size = J_size * c_out * c_in\r\n",
        "\r\n",
        "    self.radial = nn.Sequential(nn.Linear(1,32),\r\n",
        "                                nn.BatchNorm1d(32),\r\n",
        "                                nn.ReLU(),\r\n",
        "                                nn.Linear(32,32),\r\n",
        "                                nn.BatchNorm1d(32,32),\r\n",
        "                                nn.ReLU(),\r\n",
        "                                nn.Linear(32,r_size))\r\n",
        "\r\n",
        "  def forward(self, pc):\r\n",
        "    l, k = self.l, self.k\r\n",
        "    w_j = pc.get_w_j(l, k)\r\n",
        "    dist = pc.graph.edata['dist'][:,None]\r\n",
        "    size = (pc.graph.number_of_edges(), self.J_size, self.c_out, self.c_in)\r\n",
        "    R = self.radial(dist).view(*size)\r\n",
        "    w = torch.einsum('ejoi,ejlk->eoilk', R, w_j)\r\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-1fP9JqcTkZ"
      },
      "source": [
        "class AttnBlock(nn.Module):\r\n",
        "  def __init__(self, d_in, d_out, c_in=1, c_out=1):\r\n",
        "    super(AttnBlock, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.d_out = d_out\r\n",
        "\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "\r\n",
        "    self.wq = torch.randn(d_in+1, c_out, c_in, requires_grad=True)\r\n",
        "    \r\n",
        "    self.wk_layers = nn.ModuleList([\r\n",
        "                      nn.ModuleList([WLayer(k, l, c_in, c_out)\r\n",
        "                        for k in range(d_in+1)])\r\n",
        "                      for l in range(d_out+1)])\r\n",
        "\r\n",
        "  def forward(self, pc, f):\r\n",
        "    for key, value in f.items():\r\n",
        "      pc.graph.ndata[key] = value\r\n",
        "    pc.graph.ndata['q'] = self.calc_q(pc, f)\r\n",
        "\r\n",
        "    for l in range(self.d_out+1):\r\n",
        "      for k in range(self.d_in+1):\r\n",
        "        pc.graph.edata[(k,l)] = self.wk_layers[l][k](pc)\r\n",
        "\r\n",
        "    pc.graph.update_all(self.attn_msg, self.attn_rdc)\r\n",
        "    a = pc.graph.edata['exp'] / pc.graph.ndata['sum'][pc.graph.edges()[1]]\r\n",
        "    return a\r\n",
        "\r\n",
        "  def attn_msg(self, edges):\r\n",
        "    k = self.calc_k(edges)\r\n",
        "    q = edges.dst['q']\r\n",
        "    exp = torch.exp(torch.einsum('eol,eol->e',q,k))\r\n",
        "    edges.data['exp'] = exp\r\n",
        "    return {'exp': exp}\r\n",
        "\r\n",
        "  def attn_rdc(self, nodes):\r\n",
        "    # does sum over j'\r\n",
        "    exp = nodes.mailbox['exp']\r\n",
        "    sum = torch.sum(exp, dim=1)\r\n",
        "    return {'sum': sum}\r\n",
        "\r\n",
        "  def calc_q(self, pc, f):\r\n",
        "    ql = []\r\n",
        "    for k in range(min(self.d_in,self.d_out)+1):\r\n",
        "      sum = torch.einsum('oi,nik->nok',self.wq[k],f[k])\r\n",
        "      ql.append(sum)\r\n",
        "    q = torch.cat(ql, dim=2)\r\n",
        "    return q\r\n",
        "\r\n",
        "  def calc_k(self, edges):\r\n",
        "    kl = []\r\n",
        "    for l in range(min(self.d_in,self.d_out)+1):\r\n",
        "      wks = []\r\n",
        "      for k in range(self.d_in+1):\r\n",
        "        wk = torch.einsum('eoilk,eik->eol',\r\n",
        "                          edges.data[(k,l)],\r\n",
        "                          edges.dst[k])\r\n",
        "        wks.append(wk)\r\n",
        "      stack = torch.stack(wks, dim=3)\r\n",
        "      sum = torch.sum(stack, dim=3)\r\n",
        "      kl.append(sum)\r\n",
        "    k = torch.cat(kl, dim=2)\r\n",
        "    return k\r\n",
        "\r\n",
        "class TransLayer(nn.Module):\r\n",
        "  def __init__(self, d_in, d_out, c_in=1, c_out=1):\r\n",
        "    super(TransLayer, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.d_out = d_out\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "\r\n",
        "    self.wv_layers = nn.ModuleList([\r\n",
        "                        nn.ModuleList([\r\n",
        "                          WLayer(k, l, c_in=c_in, c_out=c_out)\r\n",
        "                          for l in range(d_out+1)])\r\n",
        "                        for k in range(d_in+1)])\r\n",
        "\r\n",
        "    self.block = AttnBlock(d_in, d_out, c_in=c_in, c_out=c_out)\r\n",
        "\r\n",
        "    self.si_nets = [nn.Sequential(nn.Linear(c_in**2, 2*c_in*c_out),\r\n",
        "                                 nn.BatchNorm1d(2*c_in*c_out),\r\n",
        "                                 nn.ReLU(),\r\n",
        "                                 nn.Linear(2*c_in*c_out, 2*c_in*c_out),\r\n",
        "                                 nn.BatchNorm1d(2*c_in*c_out),\r\n",
        "                                 nn.ReLU(),\r\n",
        "                                 nn.Linear(2*c_in*c_out, c_in*c_out))\r\n",
        "                   for _ in range(min(d_in, d_out)+1)]\r\n",
        "\r\n",
        "  def forward(self, pc, f):\r\n",
        "    pc.graph.edata['a'] = self.block(pc, f)\r\n",
        "    for k in range(self.d_in+1):\r\n",
        "      pc.graph.ndata[k] = f[k]\r\n",
        "      for l in range(self.d_out+1):\r\n",
        "        pc.graph.edata[(k,l)] = self.wv_layers[k][l](pc)\r\n",
        "    pc.graph.update_all(self.msg_func, self.rdc_func)\r\n",
        "    si = self.calc_si(pc, f)\r\n",
        "    f = dict()\r\n",
        "    for l in range(self.d_out+1):\r\n",
        "      data = pc.graph.ndata[l]\r\n",
        "      if l <= self.d_in:\r\n",
        "        data = data + si[l]\r\n",
        "      f[l] = data\r\n",
        "    return f\r\n",
        "\r\n",
        "  def msg_func(self, edges):\r\n",
        "    vls = dict()\r\n",
        "    for l in range(self.d_out+1):\r\n",
        "      vl = self.calc_vl(edges, l)\r\n",
        "      vls[l] = vl\r\n",
        "    return vls\r\n",
        "\r\n",
        "  def rdc_func(self, nodes):\r\n",
        "    f = dict()\r\n",
        "    for key, value in nodes.mailbox.items():\r\n",
        "      f[key] = torch.sum(value, dim=1)\r\n",
        "    return f\r\n",
        "\r\n",
        "  def calc_vl(self, edges, l):\r\n",
        "    a = edges.data['a']\r\n",
        "    vlks = []\r\n",
        "    for k in range(self.d_in+1):\r\n",
        "      wk = edges.data[(k,l)]\r\n",
        "      f = edges.src[k]\r\n",
        "      vlk = torch.einsum('e,eoilk,eik->eol', a, wk, f)\r\n",
        "      vlks.append(vlk)\r\n",
        "    vlk = torch.stack(vlks, dim=3)\r\n",
        "    vl = torch.sum(vlk, dim=3)\r\n",
        "    return vl\r\n",
        "\r\n",
        "  def calc_si(self, pc, f):\r\n",
        "    si = dict()\r\n",
        "    num_nodes = pc.graph.num_nodes()\r\n",
        "    c_in = self.c_in\r\n",
        "    c_out = self.c_out\r\n",
        "    size_in = (num_nodes, c_in*c_in)\r\n",
        "    size_out = (num_nodes, c_out, c_in)\r\n",
        "    for l in range(min(self.d_in,self.d_out)+1):\r\n",
        "      f_l = f[l]\r\n",
        "      inner = torch.einsum('ncl,ndl->ncd',f_l,f_l).view(*size_in)\r\n",
        "      si_w = self.si_nets[l](inner).view(*size_out)\r\n",
        "      si[l] = torch.einsum('noi,nil->nol', si_w, f[l])\r\n",
        "    return si\r\n",
        "\r\n",
        "class MultiHead(nn.Module):\r\n",
        "  def __init__(self, d_in, d_out, c_in=1, c_out=1, heads = 1):\r\n",
        "    super(MultiHead, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.d_out = d_out\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "    self.heads = heads\r\n",
        "    layers = [TransLayer(d_in, d_out, c_in//heads, c_out//heads)\r\n",
        "              for _ in range(heads)]\r\n",
        "    self.layers = nn.ModuleList(layers)\r\n",
        "\r\n",
        "  def forward(self, pc, f):\r\n",
        "    heads = self.heads\r\n",
        "    fs = self.split_f(f, heads)\r\n",
        "    outs = [self.layers[i](pc, fs[i]) for i in range(heads)]\r\n",
        "    out = self.merge_fs(outs)\r\n",
        "    return out\r\n",
        "\r\n",
        "  # Split features into chunks\r\n",
        "  @staticmethod\r\n",
        "  def split_f(f, heads):\r\n",
        "    fs = [dict()]*heads\r\n",
        "    for l, f_l in f.items():\r\n",
        "      chunk = torch.chunk(f_l,heads,dim=1)\r\n",
        "      for i in range(heads):\r\n",
        "        fs[i][l] = chunk[i]\r\n",
        "    return fs\r\n",
        "\r\n",
        "  # Put chunks back together\r\n",
        "  @staticmethod\r\n",
        "  def merge_fs(fs):\r\n",
        "    f = dict()\r\n",
        "    for l in fs[0].keys():\r\n",
        "      f_ls = [f_i[l] for f_i in fs]\r\n",
        "      f[l] = torch.cat(f_ls, dim=1)\r\n",
        "    return f\r\n",
        "\r\n",
        "class GraphReLU(nn.Module):\r\n",
        "  def __init__(self, d_in, channels=1):\r\n",
        "    super(GraphReLU, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.channels = channels\r\n",
        "    self.lns = nn.ModuleList([nn.LayerNorm(channels) for _ in range(d_in+1)])\r\n",
        "    self.relu = nn.ReLU()\r\n",
        "\r\n",
        "  def forward(self, f):\r\n",
        "    out = dict()\r\n",
        "    for l, f_l in f.items():\r\n",
        "      norm = torch.norm(f[l],dim=2,keepdim=True)\r\n",
        "      out[l] = self.relu(self.lns[l](norm[...,0])).unsqueeze(-1) * (f_l / norm)\r\n",
        "    return out\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao6XczOfxGX2"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZEAsAvZsy7"
      },
      "source": [
        "# Machine epsilon. Errors should be around this order of magnitude (assuming output are O(1)).\r\n",
        "torch.finfo(torch.float32).eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqSLGwlBuZQD"
      },
      "source": [
        "# Create test graphs, point clouds, and fures\r\n",
        "num_pts = 10\r\n",
        "\r\n",
        "a = 0.2\r\n",
        "b = 0.4\r\n",
        "c = 0.7\r\n",
        "\r\n",
        "in_channels=12\r\n",
        "out_channels=16\r\n",
        "\r\n",
        "num_edgs = num_pts * (num_pts-1)\r\n",
        "\r\n",
        "pos = torch.rand(num_pts,3)\r\n",
        "pc = PointCloud(pos)\r\n",
        "\r\n",
        "nf = {0: torch.rand(num_pts,in_channels,1),\r\n",
        "         1: torch.rand(num_pts,in_channels,3),\r\n",
        "         2: torch.rand(num_pts,in_channels,5)}\r\n",
        "\r\n",
        "ef = {0: torch.rand(num_edgs,in_channels,1),\r\n",
        "         1: torch.rand(num_edgs,in_channels,3),\r\n",
        "         2: torch.rand(num_edgs,in_channels,5)}\r\n",
        "\r\n",
        "d1 = torch.tensor(wigner_D_matrix(1,a,b,c)).float()\r\n",
        "\r\n",
        "pos_rot = torch.einsum('lk,nk->nl',d1,pos)\r\n",
        "pc_rot = PointCloud(pos_rot)\r\n",
        "\r\n",
        "nf_rot = dict()\r\n",
        "for key, value in nf.items():\r\n",
        "  d = torch.tensor(wigner_D_matrix(key,a,b,c)).float()\r\n",
        "  nf_rot[key] = torch.einsum('lk,nik->nil',d,value)\r\n",
        "\r\n",
        "ef_rot = dict()\r\n",
        "for key, value in ef.items():\r\n",
        "  d = torch.tensor(wigner_D_matrix(key,a,b,c)).float()\r\n",
        "  ef_rot[key] = torch.einsum('lk,eik->eil',d,value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7x9IQKd3e7z"
      },
      "source": [
        "# Make sure distances are invariant in PointCloud\r\n",
        "\r\n",
        "dist_diff = pc.graph.edata['dist'] - pc_rot.graph.edata['dist']\r\n",
        "torch.max(torch.abs(dist_diff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj5BDQYIYGyS"
      },
      "source": [
        "# Make sure vectors are equivariant in PointCloud\r\n",
        "\r\n",
        "d1 = torch.tensor(wigner_D_matrix(1,a,b,c)).float()\r\n",
        "\r\n",
        "vec = pc.graph.edata['vec']\r\n",
        "\r\n",
        "vec_post = torch.einsum('lk,ek->el',d1,vec)\r\n",
        "vec_pre = pc_rot.graph.edata['vec']\r\n",
        "\r\n",
        "diff_vec = vec_post-vec_pre\r\n",
        "\r\n",
        "torch.max(torch.abs(diff_vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8VP3GNjZhB1"
      },
      "source": [
        "# Check WLayer invariance\r\n",
        "w_layer = WLayer(0,0,in_channels,out_channels)\r\n",
        "\r\n",
        "w = w_layer(pc)\r\n",
        "w_f = torch.einsum('eoilk,eik->eol',w,ef[0])\r\n",
        "\r\n",
        "w_rot = w_layer(pc_rot)\r\n",
        "w_rot_f = torch.einsum('eoilk,eik->eol',w_rot,ef_rot[0])\r\n",
        "\r\n",
        "torch.max(torch.abs(w_f-w_rot_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn9_RqFaZoKD"
      },
      "source": [
        "# Check WLayer equivariance (0,1)\r\n",
        "w_layer = WLayer(0,1,in_channels,out_channels)\r\n",
        "\r\n",
        "d1 = torch.tensor(wigner_D_matrix(1,a,b,c)).float()\r\n",
        "\r\n",
        "w = w_layer(pc)\r\n",
        "w_f = torch.einsum('eoilk,eik->eol',w,ef[0])\r\n",
        "w_f_rot = torch.einsum('lk,eok->eol',d1,w_f)\r\n",
        "\r\n",
        "w_rot = w_layer(pc_rot)\r\n",
        "w_rot_f = torch.einsum('eoilk,eik->eol',w_rot,ef_rot[0])\r\n",
        "\r\n",
        "torch.max(torch.abs(w_f_rot-w_rot_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVFrHnz8qnt2"
      },
      "source": [
        "# Check WLayer equivariance (1,1)\r\n",
        "w_layer = WLayer(1,1,in_channels,out_channels)\r\n",
        "\r\n",
        "d1 = torch.tensor(wigner_D_matrix(1,a,b,c)).float()\r\n",
        "\r\n",
        "w = w_layer(pc)\r\n",
        "w_f = torch.einsum('eoilk,eik->eol',w,ef[1])\r\n",
        "w_f_rot = torch.einsum('lk,eok->eol',d1,w_f)\r\n",
        "\r\n",
        "w_rot = w_layer(pc_rot)\r\n",
        "w_rot_f = torch.einsum('eoilk,eik->eol',w_rot,ef_rot[1])\r\n",
        "\r\n",
        "torch.max(torch.abs(w_f_rot-w_rot_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDAYxTR0sd2b"
      },
      "source": [
        "# Check WLayer equivariance (2,3)\r\n",
        "w_layer = WLayer(2,3,in_channels,out_channels)\r\n",
        "\r\n",
        "d3 = torch.tensor(wigner_D_matrix(3,a,b,c)).float()\r\n",
        "\r\n",
        "w = w_layer(pc)\r\n",
        "w_f = torch.einsum('eoilk,eik->eol',w,ef[2])\r\n",
        "\r\n",
        "w_f_rot = torch.einsum('lk,eok->eol',d3,w_f)\r\n",
        "\r\n",
        "w_rot = w_layer(pc_rot)\r\n",
        "w_rot_f = torch.einsum('eoilk,eik->eol',w_rot,ef_rot[2])\r\n",
        "\r\n",
        "torch.max(torch.abs(w_f_rot-w_rot_f))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECdBl_ixsoIK"
      },
      "source": [
        "# Check TransLayer for equivariance\r\n",
        "layer = TransLayer(2,3,in_channels,out_channels)\r\n",
        "\r\n",
        "out = layer(pc, nf)\r\n",
        "out_rot = layer(pc_rot, nf_rot)\r\n",
        "\r\n",
        "out_post = dict()\r\n",
        "for key, value in out.items():\r\n",
        "  d = torch.tensor(wigner_D_matrix(key,a,b,c)).float()\r\n",
        "  out_post[key] = torch.einsum('lk,nik->nil',d,value)\r\n",
        "\r\n",
        "torch.max(torch.abs(out_rot[3]-out_post[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9BJ1H__2A5I"
      },
      "source": [
        "# Check MultiHead for equivariance\r\n",
        "layer = MultiHead(2,3,in_channels,out_channels,heads=4)\r\n",
        "\r\n",
        "out = layer(pc, nf)\r\n",
        "out_rot = layer(pc_rot, nf_rot)\r\n",
        "\r\n",
        "out_post = dict()\r\n",
        "for key, value in out.items():\r\n",
        "  d = torch.tensor(wigner_D_matrix(key,a,b,c)).float()\r\n",
        "  out_post[key] = torch.einsum('lk,nik->nil',d,value)\r\n",
        "torch.max(torch.abs(out_rot[3]-out_post[3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdgxofssekdW"
      },
      "source": [
        "# Check GraphReLU for equivariance\r\n",
        "layer = GraphReLU(2, channels=in_channels)\r\n",
        "\r\n",
        "out = layer(nf)\r\n",
        "out_rot = layer(nf_rot)\r\n",
        "\r\n",
        "out_post = dict()\r\n",
        "for key, value in out.items():\r\n",
        "  d = torch.tensor(wigner_D_matrix(key,a,b,c)).float()\r\n",
        "  out_post[key] = torch.einsum('lk,nik->nil',d,value)\r\n",
        "torch.max(torch.abs(out_rot[1]-out_post[1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
