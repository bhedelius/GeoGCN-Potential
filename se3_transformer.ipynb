{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "se3-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulxwtkGl_5aK"
      },
      "source": [
        "# Install libraries\r\n",
        "!pip install --pre dgl-cu101\r\n",
        "\r\n",
        "# Import libraries\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "import dgl\r\n",
        "import dgl.function as fn\r\n",
        "from dgl import DGLGraph\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from scipy.special import sph_harm as sph_harm_func\r\n",
        "\r\n",
        "# The following library has functions for clebsch-gordan and wigner-d but they \r\n",
        "# cause the session to crash\r\n",
        "\r\n",
        "#!pip install spherical\r\n",
        "#import spherical\r\n",
        "#import quaternionic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG91AePzwoUk"
      },
      "source": [
        "from numpy import sqrt\r\n",
        "from scipy.special import factorial\r\n",
        "\r\n",
        "\r\n",
        "def clebsch_gordan(j1,j2,m1,m2,J,M=None):\r\n",
        "  ''' Using equation from Wikipedia:\r\n",
        "  https://en.wikipedia.org/wiki/Table_of_Clebschâ€“Gordan_coefficients.\r\n",
        "\r\n",
        "  The equation isn't numerically stable and not fast (speed isn't too important)\r\n",
        "  TODO: Possibly find more stable implementation'''\r\n",
        "  # TODO: Replace with spherical.clebsch_gordan\r\n",
        "  if M is None:\r\n",
        "    M=m1+m2\r\n",
        "  if M<0:\r\n",
        "    return (-1)**(J-j1-j2)*clebsch_gordan(j1,j2,-m1,-m2,J,-M)\r\n",
        "  if j1<j2:\r\n",
        "    return (-1)**(J-j1-j2)*clebsch_gordan(j2,j1,m2,m1,J,M)\r\n",
        "  if not M==m1+m2:\r\n",
        "    return 0\r\n",
        "  A = sqrt((2*J+1)*factorial(J+j1-j2)*factorial(J-j1+j2)*factorial(j1+j2-J)/factorial(j1+j2+J+1))\r\n",
        "  B = sqrt(factorial(J+M)*factorial(J-M)*factorial(j1-m1)*factorial(j1+m1)*factorial(j2-m2)*factorial(j2+m2))\r\n",
        "  k_max = min([j1+j2-J,j1-m1,j2+m2])\r\n",
        "  k_min = max([0,-(J-j2+m1),-(J-j1-m2)])\r\n",
        "  C = 0\r\n",
        "  for k in range(int(k_min), int(k_max)+1):\r\n",
        "    C += (-1)**k/(factorial(k)*factorial(j1+j2-J-k)*factorial(j1-m1-k)*factorial(j2+m2-k)*factorial(J-j2+m1+k)*factorial(J-j1-m2+k))\r\n",
        "  return A*B*C\r\n",
        "\r\n",
        "def clebsch_gordan_mats(j1,j2,J):\r\n",
        "  mats = torch.zeros(2*J+1,2*j1+1,2*j2+1)\r\n",
        "  for x, m in enumerate(torch.arange(-J, J+1)):\r\n",
        "    for y, m1 in enumerate(torch.arange(-j1, j1+1)):\r\n",
        "      for z, m2 in enumerate(torch.arange(-j2, j2+1)):\r\n",
        "        mats[x,y,z] = clebsch_gordan(j1,j2,m1,m2,J)\r\n",
        "  return mats\r\n",
        "\r\n",
        "clebsch_gordan(1/2,1/2,-1/2,1/2,1,0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuHq7F20ubHF"
      },
      "source": [
        "from numpy import sqrt, cos, sin\r\n",
        "from scipy.special import factorial\r\n",
        "\r\n",
        "def wigner_small(j,b,mp,m):\r\n",
        "  A = sqrt(factorial(j+mp)*\r\n",
        "           factorial(j-mp)*\r\n",
        "           factorial(j+m)*\r\n",
        "           factorial(j-m))\r\n",
        "  B=0\r\n",
        "  for s in range(max(0,m-mp),min(j+m,j-mp)+1):\r\n",
        "    top = ((-1)**(mp-m+s)*\r\n",
        "          (cos(b/2)**(2*j+m-mp-2*s))*\r\n",
        "          (sin(b/2)**(mp-m+2*s)))\r\n",
        "    bot = (factorial(j+m-s)*\r\n",
        "          factorial(s)*\r\n",
        "          factorial(mp-m+s)*\r\n",
        "          factorial(j-mp-s))\r\n",
        "    B += top/bot\r\n",
        "  return A*B\r\n",
        "\r\n",
        "def wigner_small_mat(l, b):\r\n",
        "  mat = torch.zeros(2*l+1,2*l+1)\r\n",
        "  for i in range(2*l+1):\r\n",
        "    for j in range(2*l+1):\r\n",
        "      mat[j,i] = wigner_small(l,b,i-l,j-l)\r\n",
        "  return mat\r\n",
        "\r\n",
        "#wigner(1,np.pi/2,1,1)\r\n",
        "wigner_small_mat(1,np.pi/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDfUPE4SKm90"
      },
      "source": [
        "class PointCloud:\r\n",
        "  '''Represents a point cloud in R^3. This class calculates and stores relevant\r\n",
        "  information such as the vectors, distances, directions, and spherical\r\n",
        "  harmonics of the vectors.'''\r\n",
        "  # new_g, new_ntypes, new_etypes, new_nframes, new_eframes\r\n",
        "  def __init__(self, pos, cutoff=8.0):\r\n",
        "    edges = self._find_edges_(pos, cutoff)\r\n",
        "    self.graph = dgl.graph(edges)\r\n",
        "    self.graph.ndata['pos'] = pos\r\n",
        "    self._calc_edge_info_()\r\n",
        "    self.sph_harm = dict()\r\n",
        "    self.wj = dict()\r\n",
        "\r\n",
        "  def _find_edges_(self, pos, cutoff):\r\n",
        "    # Use positions to create graph. Need to improve! Currently O(n^2)\r\n",
        "    self.vec_mat = pos[:,None,:]-pos[None,:,:]\r\n",
        "    self.dist_mat = torch.sqrt(torch.sum((self.vec_mat)**2,axis=-1))\r\n",
        "    u = []\r\n",
        "    v = []\r\n",
        "    for j in range(len(pos)):\r\n",
        "      for i in range(j):\r\n",
        "        if self.dist_mat[i,j] < cutoff:\r\n",
        "          u.append(i)\r\n",
        "          v.append(j)\r\n",
        "    u, v = torch.tensor(u+v), torch.tensor(v+u)\r\n",
        "    return (u,v)\r\n",
        "\r\n",
        "  def _calc_edge_info_(self):\r\n",
        "    # Calculate and store position and angle information\r\n",
        "    u,v = self.graph.edges()[0], self.graph.edges()[1]\r\n",
        "    vec = self.vec_mat[u,v]\r\n",
        "    self.graph.edata['vec'] = vec\r\n",
        "    dist = self.dist_mat[u,v]\r\n",
        "    self.graph.edata['dist'] = dist\r\n",
        "    dir = vec/dist[:,None]\r\n",
        "    self.graph.edata['dir'] = dir\r\n",
        "    self.graph.edata['theta'] = torch.atan2(dir[:,1], dir[:,0])\r\n",
        "    self.graph.edata['phi'] = torch.arccos(dir[:,2])\r\n",
        "\r\n",
        "  def get_sph_harm(self, J):\r\n",
        "    # Returns spherical harmonic of order J.\r\n",
        "    if not J in self.sph_harm.keys():\r\n",
        "      m = torch.arange(-J,J+1)\r\n",
        "      theta = self.graph.edata['theta']\r\n",
        "      phi = self.graph.edata['phi']\r\n",
        "      self.sph_harm[J] = torch.real(sph_harm_func(m[None,:], J, theta[:,None], phi[:,None])).double()\r\n",
        "    return self.sph_harm[J]\r\n",
        "    \r\n",
        "  def get_wj(self, l, k):\r\n",
        "    # This needs to be improved\r\n",
        "    if not (l,k) in self.wj.keys():\r\n",
        "      wj = torch.zeros(k+l-abs(k-l)+1, self.graph.number_of_edges(), 2*l+1, 2*k+1)\r\n",
        "      for i, J in enumerate(range(abs(k-l), k+l+1)):\r\n",
        "        sh = self.get_sph_harm(J)\r\n",
        "        cg = clebsch_gordan_mats(l,k,J).double()\r\n",
        "        wj[i] = torch.einsum(\"em,mlk->elk\",sh, cg)\r\n",
        "      self.wj[(l,k)] = wj.transpose(0,1).clone()\r\n",
        "    return self.wj[(l,k)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjZzy5_3SIdK"
      },
      "source": [
        "# TODO: Self interaction\r\n",
        "# TODO: Ability to change head size\r\n",
        "\r\n",
        "# Dictionary for indices\r\n",
        "# e: edges\r\n",
        "# i: c_in\r\n",
        "# o: c_out\r\n",
        "# l: output tensor representation\r\n",
        "# k: input tensor representation\r\n",
        "# j: hidden tensor representation\r\n",
        "\r\n",
        "class WLayer(nn.Module):\r\n",
        "  def __init__(self, k, l, c_in=1, c_out=1):\r\n",
        "    super(WLayer, self).__init__()\r\n",
        "    self.k = k\r\n",
        "    self.l = l\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "\r\n",
        "    r_size = (k+l-abs(k-l)+1) * c_out * c_in\r\n",
        "\r\n",
        "    self.radial = nn.Sequential(nn.Linear(1,32),\r\n",
        "                                nn.BatchNorm1d(32),\r\n",
        "                                nn.ReLU(),\r\n",
        "                                nn.Linear(32,32),\r\n",
        "                                nn.BatchNorm1d(32,32),\r\n",
        "                                nn.ReLU(),\r\n",
        "                                nn.Linear(32,r_size))\r\n",
        "\r\n",
        "  def forward(self, pc):\r\n",
        "    wj = pc.get_wj(self.l, self.k)\r\n",
        "    R = self.radial(pc.graph.edata['dist'][:,None])\r\n",
        "    R = R.reshape((-1,\r\n",
        "                   self.k+self.l-abs(self.k-self.l)+1,\r\n",
        "                   self.c_out,\r\n",
        "                   self.c_in))\r\n",
        "    w = torch.einsum('ejoi,ejlk->eoilk',R,wj)\r\n",
        "    return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-1fP9JqcTkZ"
      },
      "source": [
        "from torch.autograd import Variable\r\n",
        "\r\n",
        "class AttBlock(nn.Module):\r\n",
        "  def __init__(self, d_in, d_out, c_in=1, c_out=1):\r\n",
        "    super(AttBlock, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.d_out = d_out\r\n",
        "\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "\r\n",
        "    self.wq = Variable(torch.randn(d_in+1,\r\n",
        "                        c_out,\r\n",
        "                        c_in),\r\n",
        "                       requires_grad=True)\r\n",
        "    \r\n",
        "    self.wk_layers = [[WLayer(k, l, c_in, c_out)\r\n",
        "                      for k in range(d_in+1)] \r\n",
        "                     for l in range(d_out+1)]\r\n",
        "\r\n",
        "  def forward(self, pc, feat):\r\n",
        "    for key, value in feat.items():\r\n",
        "      pc.graph.ndata[key] = value\r\n",
        "    q = torch.cat([torch.einsum('oi,nik->nok',self.wq[k],feat['f{}'.format(k)])\r\n",
        "                   for k in range(self.d_in+1)],\r\n",
        "                  dim=2)\r\n",
        "    pc.graph.ndata['q'] = q\r\n",
        "\r\n",
        "    for l in range(self.d_out+1):\r\n",
        "      for k in range(self.d_in+1):\r\n",
        "        pc.graph.edata['wk{}{}'.format(l,k)] = self.wk_layers[l][k](pc)\r\n",
        "\r\n",
        "    pc.graph.update_all(self.att_msg, self.att_rdc)\r\n",
        "    return pc.graph.edata['exp'], pc.graph.ndata['sum'] # return as just alpha?\r\n",
        "\r\n",
        "  def att_msg(self, edges):\r\n",
        "\r\n",
        "    K = torch.cat([\r\n",
        "            torch.sum(\r\n",
        "                torch.stack([\r\n",
        "                    torch.einsum('eoilk,eik->eol',\r\n",
        "                                edges.data['wk{}{}'.format(l,k)],\r\n",
        "                                edges.src['f{}'.format(k)])\r\n",
        "                          for k in range(self.d_in+1)],\r\n",
        "                          dim=3),\r\n",
        "                      dim=3)\r\n",
        "                   for l in range(self.d_out+1)],\r\n",
        "                  dim=2)\r\n",
        "    q = edges.dst['q']\r\n",
        "    exp = torch.exp(torch.einsum('eol,eol->e',q,K))\r\n",
        "    return {'exp': exp}\r\n",
        "\r\n",
        "  def att_rdc(self, nodes):\r\n",
        "    # does sum over j'\r\n",
        "    exp = nodes.mailbox['exp']\r\n",
        "    sum = torch.sum(exp, dim=1, keepdim=True)\r\n",
        "    return {'sum': sum}\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class TransLayer(nn.Module):\r\n",
        "  def __init__(self, d_in, d_out, c_in=1, c_out=1, heads=1):\r\n",
        "    super(TransLayer, self).__init__()\r\n",
        "    self.d_in = d_in\r\n",
        "    self.d_out = d_out\r\n",
        "    self.c_in = c_in\r\n",
        "    self.c_out = c_out\r\n",
        "    self.heads = heads\r\n",
        "\r\n",
        "    self.blocks = [AttBlock(d_in, d_dout, c_in=c_in/heads, c_out=c_out/heads)\r\n",
        "                    for _ in range(heads)]\r\n",
        "\r\n",
        "  def forward(self, pc, feat):\r\n",
        "    # pc is a point cloud\r\n",
        "    # feat is a dict\r\n",
        "    for key, value in feat.items():\r\n",
        "      pc.graph.ndata[key] = torch.reshape(-1, self.heads, self.in_channels/self.heads, value)\r\n",
        "   \r\n",
        "    pc.graph.update_all(self.att_msg, self.att_rdc)\r\n",
        "    pc.graph.update_all(self.msg_func, self.rdc_func)\r\n",
        "    return pc.graph.ndata['f']\r\n",
        "\r\n",
        "  def msg_func(self, edges):\r\n",
        "    att = edges.data['exp']/edges.dst['sum']\r\n",
        "    wv = edges.data['wv']\r\n",
        "    feat = edges.src['feat']\r\n",
        "\r\n",
        "    c = torch.einsum('ec,eclk,eck->ecl', att, wv, feat)\r\n",
        "    si = edges.dst['feat'] # Improve this!\r\n",
        "    return {'c': c+si}\r\n",
        "\r\n",
        "  def rdc_func(self,nodes):\r\n",
        "     return {'f': torch.sum(nodes.mailbox['c'], dim=1)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKd3TCy_54Q3"
      },
      "source": [
        "# Create a test graph\r\n",
        "num_pts = 10\r\n",
        "pos = torch.rand(num_pts,3)\r\n",
        "pc = PointCloud(pos)\r\n",
        "\r\n",
        "in_channels=2\r\n",
        "out_channels=3\r\n",
        "feat = {'f0': torch.rand(num_pts,in_channels,1),\r\n",
        "        'f1': torch.rand(num_pts,in_channels,3)}\r\n",
        "\r\n",
        "# AttBlock\r\n",
        "block = AttBlock(1, 1,in_channels,out_channels)\r\n",
        "print(block(pc, feat))\r\n",
        "\r\n",
        "# TFN layer\r\n",
        "#layer1 = TFNLayer(0,1,in_channels=2)\r\n",
        "#layer1(pc, feat)\r\n",
        "\r\n",
        "# Transformer layer\r\n",
        "#layer = TransLayer(0,1,channels=2)\r\n",
        "#layer(pc, feat).size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtpE3baVDpgl"
      },
      "source": [
        "\r\n",
        "# I'm going to scrap this, instead having an attention=True option in the\r\n",
        "# Transformer layer\r\n",
        "\r\n",
        "class TFNLayer(nn.Module):\r\n",
        "  def __init__(self, k, l, channels=1):\r\n",
        "    super(TFNLayer, self).__init__()\r\n",
        "    self.k = k\r\n",
        "    self.l = l\r\n",
        "    self.channels = channels\r\n",
        "    self.wlayer = WLayer(k, l, channels)\r\n",
        "    #self.self_int = nn.linear\r\n",
        "\r\n",
        "  def forward(self, pc, feat):\r\n",
        "    with pc.graph.local_scope():\r\n",
        "      pc.graph.ndata['feat'] = feat\r\n",
        "      pc.graph.edata['w'] = self.wlayer(pc)\r\n",
        "      pc.graph.update_all(self.msg_func, self.rdc_func)\r\n",
        "      return pc.graph.ndata['f']\r\n",
        "\r\n",
        "  def msg_func(self, edges):\r\n",
        "    print(edges.data['w'].size(), edges.src['feat'].size())\r\n",
        "    return {'m': torch.einsum('eclk,eck->ecl',edges.data['w'],edges.src['feat'])}\r\n",
        "\r\n",
        "  def rdc_func(self,nodes):\r\n",
        "     return {'f': torch.sum(nodes.mailbox['m'], dim=1)}"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
